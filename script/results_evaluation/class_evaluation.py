import re
from rdflib import Graph, Namespace, URIRef
import random
from typing import List, Tuple, Optional, Dict
from rdflib import Graph, Namespace
import urllib.parse


def extract_by_target(target_name, rdf_file='kg/output.ttl'):

    # Load the graph
    g = Graph()
    g.parse(rdf_file, format='turtle')
    
    # Define namespaces
    DUL = Namespace('http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#')
    STER = Namespace('http://www.semanticweb.org/stereoGraph#')
    
    # Sanitize target_name to ensure it's URI-safe
    safe_target = urllib.parse.quote(str(target_name))
    target_uri = STER[safe_target]

    #if(target_name in [])

    print(target_name)
    # Debug print
    print(f"Querying for target: {target_uri}")
    
    query = f"""
    PREFIX dul: <http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#>
    PREFIX ster: <http://www.semanticweb.org/stereoGraph#>

    SELECT ?s ?stereotype
    WHERE {{
      ?s a dul:Situation ;
         dul:hasTarget <{target_uri}> ;
         ster:hasStereotype ?stereotype .
    }}
    """

    # Execute the query
    results = g.query(query)

    # Collect and return results
    extracted = []
    for row in results:
        extracted.append({
            "target": str(target_uri).split("#")[1],
            "stereotype": str(row.stereotype).split("#")[1],
        })
    return extracted

   



def retrieve_context_from_graph(text_dataset, post_text, i, sample_size, LENGUAGE):

    #parametro numero medio 

    escaped_text = re.escape(post_text)

    if LENGUAGE=="ITA":

        kg_name="kg/output_ITA.ttl"

    if LENGUAGE=="ENG":

        kg_name="kg/output_ENG.ttl"





    kg_data = extract_by_target(text_dataset['normalized_target'][i], kg_name)
    context_pieces=[]
    
    if sample_size<len(kg_data):
        kg_data = random.sample(kg_data, sample_size)
    for text in kg_data:

        
        context_pieces.append(str(text['target'])+" has stereotype "+str(text['stereotype']))

    return " ; ".join(context_pieces) if context_pieces else "Nessun contesto rilevante trovato nel grafo."
def sentence_similarity_via_prompt(chat, sentence1, sentence2):
    """
    Uses the LLaMA model via prompt to estimate similarity between two sentences.
    
    Args:
        chat: An instance of OllamaChat (already initialized with LLaMA).
        sentence1 (str): First sentence.
        sentence2 (str): Second sentence.
    
    Returns:
        float: Similarity score from 0.0 to 1.0 as interpreted by the model.
    """
    prompt = f"""Compare the following two sentences and return a similarity score from 0.0 (completely different) to 1.0 (identical in meaning). 
Only return the number.

Sentence A: "{sentence1}"
Sentence B: "{sentence2}"

Similarity score:"""
    
    response = chat.send_prompt(prompt, prompt_uuid="similarity_check", use_history=False, stream=False)
    
    # Try to extract a float score from the response
    try:
        return float(response.raw_text.strip())
    except ValueError:
        print("Invalid response from model:", response.raw_text)
        return None
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def sentence_bleu_score(reference, candidate):
    """
    Compute BLEU score between a reference sentence and a candidate sentence.
    
    Args:
        reference (str): The ground truth sentence.
        candidate (str): The sentence generated by the model.
    
    Returns:
        float: BLEU score between 0.0 and 1.0.
    """
    reference_tokens = reference.strip().split()
    candidate_tokens = candidate.strip().split()

    # BLEU expects list of references
    smoothie = SmoothingFunction().method4
    score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)
    return score



from sentence_transformers import SentenceTransformer, util

# Load a pre-trained model for sentence embeddings

similarity_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

def sentence_similarity(sentence1, sentence2):
    """
    Compute semantic similarity between two sentences using cosine similarity on sentence embeddings.
    
    Args:
        sentence1 (str): First sentence.
        sentence2 (str): Second sentence.
    
    Returns:
        float: Similarity score between 0 (completely different) and 1 (identical).
    """
    # Encode the sentences
    embeddings = similarity_model.encode([sentence1, sentence2], convert_to_tensor=True)
    
    # Compute cosine similarity
    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1])
    
    return similarity_score.item()


def evaluate_all_similarities(
    sentence_pairs: List[Tuple[str, str]],
    chat=None  # Optional for LLM prompt similarity
) -> Dict[str, Dict[str, Optional[float]]]:
    """
    Applies BLEU, SBERT, and ROUGE-L similarity functions to each sentence pair and computes average scores.
    """
    bleu_scores = []
    embedding_scores = []
    rouge_l_scores = []

    for s1, s2 in sentence_pairs:
        bleu = sentence_bleu_score(s1, s2)
        emb = sentence_similarity(s1, s2)
        rouge = sentence_rouge_l(s1, s2)

        bleu_scores.append(bleu)
        embedding_scores.append(emb)
        rouge_l_scores.append(rouge)

    def average(scores: List[Optional[float]]) -> Optional[float]:
        valid = [s for s in scores if s is not None]
        return sum(valid) / len(valid) if valid else None

    return {
        'bleu': {
            'scores': bleu_scores,
            'average': average(bleu_scores)
        },
        'embedding': {
            'scores': embedding_scores,
            'average': average(embedding_scores)
        },
        'rougeL': {
            'scores': rouge_l_scores,
            'average': average(rouge_l_scores)
        }
    }


    def average(scores: List[Optional[float]]) -> Optional[float]:
        valid = [s for s in scores if s is not None]
        return sum(valid) / len(valid) if valid else None

    return {
        'bleu': {
            'scores': bleu_scores,
            'average': average(bleu_scores)
        },
        'embedding': {
            'scores': embedding_scores,
            'average': average(embedding_scores)
        }
    }
from rouge_score import rouge_scorer

def sentence_rouge_l(reference: str, candidate: str) -> float:
    """
    Compute ROUGE-L score between a reference sentence and a candidate sentence.
    
    Args:
        reference (str): The ground truth sentence.
        candidate (str): The sentence generated by the model.
    
    Returns:
        float: ROUGE-L F1 score between 0.0 and 1.0.
    """
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    score = scorer.score(reference, candidate)
    return score['rougeL'].fmeasure


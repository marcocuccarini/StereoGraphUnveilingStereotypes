{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f47ab0-e3b3-46af-880b-b5535eab517d",
   "metadata": {},
   "source": [
    "# From test prompts to LLM answers\n",
    "\n",
    "This notebook take in in put set of argumentative tree, and using the rule defined on the paper extract the support from the attack. In this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60682202-c386-40b4-8d0c-ab1dafbe8495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.2 (v3.13.2:4f8bb3947cf, Feb  4 2025, 11:51:10) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "punkt ðŸ«¡\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from classes.llms_classes import *\n",
    "from classes.prompt_classes import *\n",
    "from classes.dataset_classes import TextDataset\n",
    "from classes.matrix_creation_classes import *\n",
    "from prompt import *\n",
    "import re\n",
    "import random\n",
    "\n",
    "%pip install nltk\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print('punkt ðŸ«¡')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "from class_evaluation import *\n",
    "\n",
    "\n",
    "LENGUAGE=\"ITA\"   # define the langauge of the test, \"ITA\" for italian, \"ENG\" for english \n",
    "\n",
    "text_dataset=pd.read_csv(\"../data/\"+str(LENGUAGE)+\"/test_set.csv\", sep=\";\", on_bad_lines='skip')\n",
    "\n",
    "#few_shot, dataframe_prompt, samples few_shot\n",
    "\n",
    "prompt=PromptCreation(True, \"data_prompt_contruction/test_prompts.csv\",\"data_prompt_contruction/samples_fewshot.csv\")\n",
    "\n",
    "resulting_prompt=prompt.prompt_creation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b95699",
   "metadata": {},
   "source": [
    "### Test per diversi modelli: baseline VS kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f50fb5-82fb-4caf-aa71-bfb61081715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: ['llama3.1:latest', 'mistral:latest', 'llama3.1:8b', 'nomic-embed-text:latest', 'qwen2.5:latest', 'deepseek-r1:7b', 'llama3.2:3b', 'gemma2:2b', 'gemma2:latest', 'phi3.5:latest', 'qwen2:latest', 'phi3:latest', 'tinyllama:latest', 'qwen2:1.5b', 'qwen2:0.5b', 'llama3:latest']\n",
      "llama3.1 is already available in the server\n",
      "llama3.1 is already available in the server\n",
      "\n",
      "--- Test dell'approccio BASELINE con modello: llama3.1 ---\n",
      "################## RESPONSE #####################\n",
      "\n",
      "IMPLICIT STATEMENT PREDICTED:\n",
      "gli italiani sono vittime del problema dei campi nomadi.\n",
      "\n",
      "IMPLICIT STATEMENT DATASET:\n",
      "tutti i rom sono pericolosi\n",
      "================================================\n",
      "\n",
      "################## RESPONSE #####################\n",
      "\n",
      "IMPLICIT STATEMENT PREDICTED:\n",
      "i migranti portano malattie.\n",
      "\n",
      "IMPLICIT STATEMENT DATASET:\n",
      "i migranti portano malattie\n",
      "================================================\n",
      "\n",
      "################## RESPONSE #####################\n",
      "\n",
      "IMPLICIT STATEMENT PREDICTED:\n",
      "i terroristi sono degli invasori.\n",
      "\n",
      "IMPLICIT STATEMENT DATASET:\n",
      "i migranti sono criminali\n",
      "================================================\n",
      "\n",
      "################## RESPONSE #####################\n",
      "\n",
      "IMPLICIT STATEMENT PREDICTED:\n",
      "i rom disturbano le persone.\n",
      "\n",
      "IMPLICIT STATEMENT DATASET:\n",
      "i rom sono mendicanti\n",
      "================================================\n",
      "\n",
      "################## RESPONSE #####################\n",
      "\n",
      "IMPLICIT STATEMENT PREDICTED:\n",
      "gli stranieri delinquono.\n",
      "\n",
      "IMPLICIT STATEMENT DATASET:\n",
      "gli stranieri sono criminali\n",
      "================================================\n",
      "\n",
      "\n",
      "--- Test dell'approccio KG con modello: llama3.1 ---\n",
      "rom_sinti\n",
      "Querying for target: http://www.semanticweb.org/stereoGraph#rom_sinti\n",
      "immigrati\n",
      "Querying for target: http://www.semanticweb.org/stereoGraph#immigrati\n",
      "immigrati\n",
      "Querying for target: http://www.semanticweb.org/stereoGraph#immigrati\n",
      "rom_sinti\n",
      "Querying for target: http://www.semanticweb.org/stereoGraph#rom_sinti\n",
      "minoranza_etnica\n",
      "Querying for target: http://www.semanticweb.org/stereoGraph#minoranza_etnica\n"
     ]
    }
   ],
   "source": [
    "# Init the ollama server\n",
    "ollama_server = OllamaServer(ollama)\n",
    "\n",
    "# Check models that have already been downloaded\n",
    "models = ollama_server.get_models_list()\n",
    "print(\"Available Models:\", models)\n",
    "\n",
    "MODELS = ['llama3.1']\n",
    "#MODELS = ['mistral:7b']\n",
    "#MODELS = ['qwen3:8b']\n",
    "#MODELS = ['llama3.1:8b', 'mistral:7b', 'qwen3:8b']\n",
    "\n",
    "EXPERIMENTS = ['baseline', 'kg']          \n",
    "\n",
    "sample_test = 5 #quanti esempi testare \n",
    "sample_size= 20 # Numero massimo di esempi froniti dal kg (funzione di cui ne parlava Lia)\n",
    "all_results = {}\n",
    "\n",
    "#---------  SET UP OLLAMA + SELEZIONE MODELLO ---------------------\n",
    "\n",
    "for MODEL in MODELS:\n",
    "    all_results[MODEL] = {}\n",
    "\n",
    "    # Download the model to use for experiences\n",
    "    models = ollama_server.download_model_if_not_exists(MODEL)\n",
    "\n",
    "    # Initialize chat with a specific model\n",
    "    chat = OllamaChat(server=ollama_server, model=MODEL)\n",
    "\n",
    "# --------------- APPROCCIO BASELINE -----------------------------\n",
    "    print(f\"\\n--- Test dell'approccio BASELINE con modello: {MODEL} ---\")\n",
    "\n",
    "    response_baseline = []\n",
    "\n",
    "    # Create the object for the creation of the support matrix\n",
    "    for i in range(len(text_dataset['post'][:sample_test])):\n",
    "        post_text = text_dataset['post'][i]\n",
    "        implied_statement = text_dataset['implied_statement'][i]\n",
    "\n",
    "        # Format the prompt\n",
    "        #prompt = resulting_prompt.to_list()[0].format(post_text)\n",
    "\n",
    "        prompt2_baseline = costruisci_prompt3(post_text)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            response = chat.send_prompt(prompt2_baseline, prompt_uuid=\"1\", use_history=False) #, stream=True)\n",
    "            response_text_baseline = response.raw_text.strip()\n",
    "        except Exception as e:\n",
    "            response_text_baseline = f\"[Error during prompt generation: {e}]\"\n",
    "\n",
    "        # Store response\n",
    "        response_baseline.append(response_text_baseline)\n",
    "\n",
    "        #Print response and reference\n",
    "        print(\"################## RESPONSE #####################\\n\")\n",
    "        print(\"IMPLICIT STATEMENT PREDICTED:\")\n",
    "        print(response_text_baseline)\n",
    "        print(\"\\nIMPLICIT STATEMENT DATASET:\")\n",
    "        print(implied_statement)\n",
    "        print(\"================================================\\n\")\n",
    "\n",
    "    #store model-specific baseline responses\n",
    "    all_results[MODEL]['baseline'] = response_baseline\n",
    "\n",
    "# --------------- APPROCCIO CON KG -------------------------------------------\n",
    "    # Ora il ciclo con integrazione del contesto RDF\n",
    "    print(f\"\\n--- Test dell'approccio KG con modello: {MODEL} ---\")\n",
    "    response_kg = []\n",
    "\n",
    "    for i in range(len(text_dataset['post'][:sample_test])):\n",
    "        post_text = text_dataset['post'][i]\n",
    "        implied_statement = text_dataset['implied_statement'][i]\n",
    "\n",
    "        # Recupera contesto dal grafo RDF\n",
    "        context = retrieve_context_from_graph(text_dataset, post_text, i, sample_size, LENGUAGE)\n",
    "\n",
    "        # Costruisci il prompt con contesto\n",
    "        prompt2 = build_prompt3(post_text, context)\n",
    "\n",
    "\n",
    "        try:\n",
    "            response = chat.send_prompt(prompt2, prompt_uuid=\"1\", use_history=False) #, stream=True)\n",
    "            response_text_kg = response.raw_text.strip()\n",
    "        except Exception as e:\n",
    "            response_text_kg = f\"[Errore durante la generazione della risposta: {e}]\"\n",
    "\n",
    "        #store resp\n",
    "        response_kg.append(response_text_kg)\n",
    "\n",
    "        #print(\"################## RESPONSE #####################\")\n",
    "        #print(\"IMPLICIT STATEMENT PREDICTED:\")\n",
    "        #print(response_text_kg)\n",
    "        #print(\"\\nIMPLICIT STATEMENT DATASET:\")\n",
    "        #print(implied_statement)\n",
    "        #print(\"================================================\\n\")\n",
    "\n",
    "    #store response from kg approach    \n",
    "    all_results[MODEL]['kg'] = response_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c217ccbb-82ed-4060-8c95-24db56de8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00f362b-e271-4d0f-b05d-f640984c9a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ EVALUATION FOR MODEL: llama3.1 ================\n",
      "\n",
      "----------- BASELINE RESULTS -----------\n",
      "BLEU scores: [0.02573285025273419, 0.43146827293898643, 0.05428693985879238, 0.12121093525642128, 0.15174681566793558]\n",
      "BLEU average: 0.1569\n",
      "EMBEDDING scores: [0.48159393668174744, 0.9895774722099304, 0.5093135833740234, 0.6415661573410034, 0.9412040114402771]\n",
      "EMBEDDING average: 0.7127\n",
      "\n",
      "\n",
      "----------- KG RESULTS -----------\n",
      "BLEU scores: [0, 0.43146827293898643, 0.044706344276931285, 0.23376641384792204, 0.5081327481546147]\n",
      "BLEU average: 0.2436\n",
      "EMBEDDING scores: [0.40563341975212097, 0.9895774722099304, 0.5275911092758179, 0.7340084910392761, 0.8965467214584351]\n",
      "EMBEDDING average: 0.7107\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for MODEL in MODELS:\n",
    "    print(f\"\\n================ EVALUATION FOR MODEL: {MODEL} ================\\n\")\n",
    "\n",
    "    # Retrieve stored responses\n",
    "    response_baseline = all_results[MODEL]['baseline']\n",
    "    response_kg = all_results[MODEL]['kg']\n",
    "\n",
    "    # Ensure alignment with sample_test\n",
    "    true_statements = text_dataset['implied_statement'][:sample_test]\n",
    "\n",
    "    # Pair gold labels with predictions\n",
    "    sentence_base = list(zip(true_statements, response_baseline))\n",
    "    sentence_kg = list(zip(true_statements, response_kg))\n",
    "\n",
    "    # --- Baseline Evaluation ---\n",
    "    print(\"----------- BASELINE RESULTS -----------\")\n",
    "    results_baseline = evaluate_all_similarities(sentence_base, chat)\n",
    "    for method, result in results_baseline.items():\n",
    "        print(f\"{method.upper()} scores: {result['scores']}\")\n",
    "        print(f\"{method.upper()} average: {result['average']:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # --- KG Evaluation ---\n",
    "    print(\"----------- KG RESULTS -----------\")\n",
    "    results_kg = evaluate_all_similarities(sentence_kg, chat)\n",
    "    for method, result in results_kg.items():\n",
    "        print(f\"{method.upper()} scores: {result['scores']}\")\n",
    "        print(f\"{method.upper()} average: {result['average']:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0437bc1-b5ed-4736-840a-019aafa5fb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Exporting results for MODEL: llama3.1\n",
      "âœ… JSON file 'result/prediction_results_llama3.1_prompt3_ITA.json' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for MODEL in MODELS:\n",
    "    print(f\"\\n>>> Exporting results for MODEL: {MODEL}\")\n",
    "\n",
    "    # Retrieve predictions\n",
    "    baseline_responses = all_results[MODEL]['baseline']\n",
    "    kg_responses = all_results[MODEL]['kg']\n",
    "\n",
    "    # Ensure ground-truth and KG context samples are aligned\n",
    "    implied_statements = text_dataset['implied_statement'][:sample_test]\n",
    "\n",
    "    # Pair gold labels with predictions for evaluation\n",
    "    sentence_kg = list(zip(implied_statements, kg_responses))\n",
    "    results_kg_eval = evaluate_all_similarities(sentence_kg, chat)\n",
    "\n",
    "    # NOTE: you must prepare a list of KG context samples used for each item\n",
    "    # For example, during the KG run:\n",
    "    # all_results[MODEL]['kg_samples'] = list_of_kg_contexts\n",
    "    kg_samples = all_results[MODEL].get('kg_samples', [\"\"] * sample_test)\n",
    "\n",
    "    # Construct data to export\n",
    "    combined_data = []\n",
    "    cont=0\n",
    "    for implied, baseline, kg, kg_sample in zip(implied_statements, baseline_responses, kg_responses, kg_samples):\n",
    "        \n",
    "        combined_data.append({\n",
    "            \"implied_statement\": implied,\n",
    "            \"baseline_response\": baseline,\n",
    "            \"kg_response\": kg,\n",
    "            #\"kg_sample\": kg_sample,\n",
    "            #\"kg_sample_size\": len(kg_sample.split(\"\\n\")) if isinstance(kg_sample, str) else 0,\n",
    "            \"bleu_similarity_baseline\": str(results_baseline['bleu']['scores'][cont]),\n",
    "            \"bert_similarity_baseline\": str(results_baseline['embedding']['scores'][cont]),\n",
    "            \"bleu_similarity_kg\": str(results_kg_eval['bleu']['scores'][cont]),\n",
    "            \"bert_similarity_kg\": str(results_kg_eval['embedding']['scores'][cont])\n",
    "        })\n",
    "        cont+=1\n",
    "\n",
    "    # Save to JSON\n",
    "    file_name = f\"result/prediction_results_{MODEL.replace(':', '_')}_prompt3_\"+str(LENGUAGE)+\".json\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… JSON file '{file_name}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c45d7-ae24-4118-a2cd-f59f37883351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
